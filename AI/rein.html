<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Document</title>
</head>
<body>
    <h1>マルコフ決定過程に基づく強化学習で使う概念</h1>
    <ol>
        <li>action<br>エージェントが環境に働きかけること。複数ある行動の選択肢から一つの行動を選択する。</li>
        <li>state<br>エージェントが置かれた環境。</li>
        <li>reward<br>エージェントが受け取る報酬。この報酬を元に最適な学習をしていく。</li>
        <li>policy<br>エージェントが一番効率的に報酬を得る上で取る行動を定めたルール。</li>
        <li>時刻tでの収益Gtは時刻t+1以降の報酬累積和であり、0<=γ<=1を満たす割引率γを用いると、Gt=(Rt+1)+γ(Rt+2)+γ^2(Rt+3)+…</li>
    </ol>
    <h1>強化学習の手法</h1>
    <li>強化学習の手法は以下の３つに大別される。</li>
    <ol>
        <li>動的計画法<br>環境の完全なモデルがマルコフ決定過程として与えられてる場合のみに適用できる。<br>
        環境の完全なモデルとは</li>
        <li>モンテカルロ法</li>
        <li>TD学習<br>目標の価値と現在の価値のズレを修正する。<ol><li>Sarsa<br>
        行動価値観数を更新する際、行動価値の小さい探索結果も反映されやすい。<br>計算が不安定になりやすい。<br>方策オン型の手法である。すなわち、
        行動を決定する方策と行動価値観数に利用される方策が同じ。</li>
        <li>G学習</li><br>Sarsaと異なり、行動価値観数Qの更新が行動の決定方法に依存しない。<br>Sarsaよりも行動価値観数の収束が早くなることが多い。</ol></li>
    </ol>
    <h1>Q学習</h1>
    <li>Q-Tableの各値が最適化されるように学習する。ではQ値を更新するにはどのようにすれば良いか。<br>
    Q値=学習係数(0.1や0.01が多い)*(報酬+割引率*次の状態で最大のQ値-現在のQ値)</li>
    <li>このように式を設定することでGoalからStartに向かって報酬が伝播されていき、適切なQ値が各マスに分布されるようになっていく。
    </li>
    <li>割引率をかける理由<br>ゴールに近いほどQ値は大きくなる。割引率は将来の価値をどれくらい重要視するかを意味する。割引率γは0.9~0.99の範囲にすることが多い。</li>
    <li>Q学習のデメリット<br>Q-Tableが大きくなりすぎると上手く学習が進まなくなる。これを改善したのが次の深層強化学習。</li>
    <h1>深層強化学習</h1>
    <li>NNを使う。</li>
    <h1>ε-greedy法</h1>
    <li>学習初期にランダムな行動を混ぜることで、局所解（狭い領域における最適な解）に陥ることを防ぐ。</li>
    <li>確率εでランダムな行動、確率1-ε（Q学習における通常の行動）でQ値が最大な行動を取る。εの値を学習を重ねることで値が小さくなるようにする。<br>
    ⇔Q値が最大になる⇔最適化された行動になる</li>
</body>
</html>